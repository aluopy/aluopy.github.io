---
title: "使用 kubeadm 快速部署 k8s 集群"
permalink: /kubernetes/kubeadm-install-k8s/
excerpt: "Kubeadm 是一个快速创建 Kubernetes 集群的工具。它执行必要的操作，以用户友好的方式启动和运行最小可行、安全的集群。 Kubeadm 的范围仅限于本地节点文件系统和 Kubernetes API，它旨在成为更高级别工具的可组合构建块。"
toc: true
categories: kubernetes
tags:
  - kubernetes
  - install
  - kubeadm
---

Kubeadm 是一个快速创建 Kubernetes 集群的工具。它执行必要的操作，以用户友好的方式启动和运行最小可行、安全的集群。 Kubeadm 的范围仅限于本地节点文件系统和 Kubernetes API，它旨在成为更高级别工具的可组合构建块。

## Common Kubeadm cmdlets

1. **kubeadm init** 引导初始 Kubernetes 控制平面节点。
2. **kubeadm join** 引导 Kubernetes 工作节点或其他控制平面节点，并将其加入集群。
3. **kubeadm upgrade** 将 Kubernetes 集群升级到更新的版本。
4. **kubeadm reset** 恢复 kubeadm init 或 kubeadm join 对此主机所做的任何更改。

## 准备开始

- 一台或多台兼容的 Linux 主机
- 每台机器 2 GB 或更多的 RAM
- 2 CPU 核或更多
- 集群中的所有机器的网络彼此均能相互连接（公网和内网都可以）
- 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见[这里](#确保每个节点上 MAC 地址和 product_uuid 的唯一性)了解更多详细信息。
- 开启机器上的某些端口（或直接关闭防火墙）。请参见[这里](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports) 了解更多详细信息。
- 禁用交换分区。为了保证 kubelet 正常工作，你 **必须** 禁用交换分区。

## 确保 MAC 地址和 product_uuid 的唯一性

- 可以使用命令 `ip link` 或 `ifconfig -a` 来获取网络接口的 MAC 地址
- 可以使用 `sudo cat /sys/class/dmi/id/product_uuid` 命令对 product_uuid 校验

一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。 Kubernetes 使用这些值来唯一确定集群中的节点。 如果这些值在每个节点上不唯一，可能会导致安装[失败](https://github.com/kubernetes/kubeadm/issues/31)。

## 检查网络适配器

如果你有一个以上的网络适配器，同时你的 Kubernetes 组件通过默认路由不可达，我们建议你预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。

## 允许 iptables 检查桥接流量

确保 `br_netfilter` 模块被加载。这一操作可以通过运行 `lsmod | grep br_netfilter` 来完成。若要显式加载该模块，可执行 `sudo modprobe br_netfilter`。

为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的 `sysctl` 配置中将 `net.bridge.bridge-nf-call-iptables` 设置为 1。例如：

```shell
$ sudo cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

$ sudo cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
$ sudo sysctl --system
```

更多的相关细节可查看[网络插件需求](https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements)页面。

## 检查所需端口

内网也可以直接关闭防火墙

***控制平面节点***

| 协议 | 方向 | 端口范围  | 作用                    | 使用者                       |
| ---- | ---- | --------- | ----------------------- | ---------------------------- |
| TCP  | 入站 | 6443      | Kubernetes API 服务器   | 所有组件                     |
| TCP  | 入站 | 2379-2380 | etcd 服务器客户端 API   | kube-apiserver, etcd         |
| TCP  | 入站 | 10250     | Kubelet API             | kubelet 自身、控制平面组件   |
| TCP  | 入站 | 10251     | kube-scheduler          | kube-scheduler 自身          |
| TCP  | 入站 | 10252     | kube-controller-manager | kube-controller-manager 自身 |

***工作节点***

| 协议 | 方向 | 端口范围    | 作用           | 使用者                     |
| ---- | ---- | ----------- | -------------- | -------------------------- |
| TCP  | 入站 | 10250       | Kubelet API    | kubelet 自身、控制平面组件 |
| TCP  | 入站 | 30000-32767 | NodePort 服务† | 所有组件                   |

## 禁用交换分区

```shell
# 关闭 swap
$ sudo swapoff -a  # 临时
$ sudo sed -ri 's/.*swap.*/#&/' /etc/fstab    # 永久
```

## 添加主机映射

本文部署使用 3个节点，1个 master 节点，2个 work 节点

```shell
$ cat >> /etc/hosts << EOF
192.168.20.17 m1
192.168.20.18 w1
192.168.20.19 w2
EOF
```

## 禁用 SELinux 

将 SELinux 设置为 permissive 模式（相当于将其禁用），禁用 SELinux 是允许容器访问主机文件系统所必需的，而这些操作是为了例如 Pod 网络工作正常。

```shell
$ sudo setenforce 0
$ sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
```

## 安装 runtime

### 安装 Docker

```shell
$ sudo curl -sSL https://get.daocloud.io/docker | sh
$ sudo systemctl enable docker --now
```

### 配置 cgroup 驱动程序

容器运行时和 kubelet 都具有名字为 ["cgroup driver"](https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/) 的属性，该属性对于在 Linux 机器上管理 CGroups 而言非常重要。**需要确保容器运行时和 kubelet 所使用的是相同的 cgroup 驱动**，否则 kubelet 进程会失败。

#### 查看 docker 的 cgroup driver

```shell
$ sudo docker info | grep "Cgroup Driver"
Cgroup Driver: cgroupfs
```

#### 配置 docker 的 cgroup driver

通过 `daemon.json` 文件配置，添加 `"exec-opts": ["native.cgroupdriver=systemd"]` 参数

```shell
$ sudo mkdir -p /etc/docker
$ sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://1rzb6rqw.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF

$ sudo systemctl restart docker
$ sudo docker info | grep "Cgroup Driver"
 Cgroup Driver: systemd
```

## 安装 kubeadm、kubelet 和 kubectl

需要在每台节点上安装以下的软件包：

- `kubeadm`：用来初始化集群的指令。
- `kubelet`：在集群中的每个节点上用来启动 Pod 和容器等。
- `kubectl`：用来与集群通信的命令行工具。

kubeadm **不能**帮你安装或者管理 `kubelet` 或 `kubectl`，所以你需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配。 如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。 然而，控制平面与 kubelet 间的相差一个次要版本不一致是支持的，但 kubelet 的版本不可以超过 API 服务器的版本。 例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。

> 有关安装 `kubectl` 的信息，请参阅[安装和设置 kubectl](https://kubernetes.io/zh/docs/tasks/tools/) 文档。

添加阿里云 YUM 软件源

```shell
$ sudo cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

安装 kubeadm、kubelet 和 kubectl

```shell
$ sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
$ sudo systemctl enable --now kubelet
```

以上安装命令默认安装最新版本，安装指定版本：`yum install -y kubelet-1.20.0 kubeadm-1.20.0 kubectl-1.20.0`。

## 配置 kubelet 的 cgroup driver

kubeadm 支持在执行 `kubeadm init` 时，传递一个 `KubeletConfiguration` 结构体。 `KubeletConfiguration` 包含 `cgroupDriver` 字段，可用于控制 kubelet 的 cgroup 驱动。

> **说明：** 在版本 1.22 中，如果用户没有在 `KubeletConfiguration` 中设置 `cgroupDriver` 字段， `kubeadm init` 会将它设置为默认值 `systemd`。

本文安装的 kubelet 版本为 V1.23.1，cgroup driver 默认为 systemd，前面已经将 docker 的 cgroup driver 修改为 systemd，所以不需要再修改 kubelet 的 cgroup driver，以下只是修改示例。

这是一个最小化的示例，其中显式的配置了此字段：

```yaml
# kubeadm-config.yaml
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta3
kubernetesVersion: v1.23.1
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
```

这样一个配置文件就可以传递给 kubeadm 命令了：

```shell
kubeadm init --config kubeadm-config.yaml
```

> **说明：**Kubeadm 对集群所有的节点，使用相同的 `KubeletConfiguration`。 `KubeletConfiguration` 存放于 `kube-system` 命名空间下的某个 [ConfigMap](https://kubernetes.io/zh/docs/concepts/configuration/configmap) 对象中。执行 `init`、`join` 和 `upgrade` 等子命令会促使 kubeadm 将 `KubeletConfiguration` 写入到文件 `/var/lib/kubelet/config.yaml` 中， 继而把它传递给本地节点的 kubelet。

## [使用 kubeadm 创建集群](https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/)

### 目标

- 安装单个控制平面的 Kubernetes 集群
- 初始化 Kubernetes 工作节点并将其加入集群
- 在集群上安装 Pod 网络，以便你的 Pod 可以相互连通

### 准备所需的容器镜像

这个步骤是可选的，只适用于你希望 `kubeadm init` 和 `kubeadm join` 不去下载存放在 `k8s.gcr.io` 上的默认的容器镜像的情况。

当你在离线的节点上创建一个集群的时候，Kubeadm 有一些命令可以帮助你预拉取所需的镜像。 阅读[离线运行 kubeadm](https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images) 获取更多的详情。

Kubeadm 允许你给所需要的镜像指定一个自定义的镜像仓库。 阅读[使用自定义镜像](https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images) 获取更多的详情。

### 初始化控制平面节点

```shell
$ kubeadm init \
  --apiserver-advertise-address=192.168.20.17 \
  --image-repository registry.aliyuncs.com/google_containers \
  --kubernetes-version v1.23.1 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16
```

`kubeadm init` 首先运行一系列预检查以确保机器 准备运行 Kubernetes。这些预检查会显示警告并在错误时退出。然后 `kubeadm init` 下载并安装集群控制平面组件。这可能会需要几分钟。 完成之后你应该看到：

```shell
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>
```

> 记录 `kubeadm init` 输出的 `kubeadm join` 命令。 你需要此命令[将节点加入集群](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#join-nodes)。

要使非 root 用户可以运行 kubectl，请运行以下命令， 它们也是 `kubeadm init` 输出的一部分：

```bash
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

或者，如果你是 `root` 用户，则可以运行：

```bash
$ export KUBECONFIG=/etc/kubernetes/admin.conf
```

永久生效

```bash
$ echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /etc/profile
$ source /etc/profile
```

> **警告：**
>
> kubeadm 对 `admin.conf` 中的证书进行签名时，将其配置为 `Subject: O = system:masters, CN = kubernetes-admin`。 `system:masters` 是一个例外的、超级用户组，可以绕过鉴权层（例如 RBAC）。 不要将 `admin.conf` 文件与任何人共享，应该使用 `kubeadm kubeconfig user` 命令为其他用户生成 kubeconfig 文件，完成对他们的定制授权。

### [初始化工作节点并将其加入集群](https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-join/)

初始化 Kubernetes 工作节点并将其加入集群，在规划的工作节点上执行在初始化控制平面节点时（kubeadm init）输出的kubeadm join命令：

```bash
$ kubeadm join 192.168.56.56:6443 --token r2d7kk.rk7ubjqa9rcqpkd4 \
	--discovery-token-ca-cert-hash sha256:f143936541e378e2c72e1d96d39da01b73751a1498d527d8f5d0ecb4bb220873
```

> 默认 token 有效期为24小时，当过期之后，该 token 就不可用了，这时就需要重新创建 token： `kubeadm token create --print-join-command`

## 安装 Pod 网络附加组件

> **注意：**
>
> 本节包含有关网络设置和部署顺序的重要信息。 在继续之前，请仔细阅读所有建议。
>
> **你必须部署一个基于 Pod 网络插件的 [容器网络接口](https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni) (CNI)，以便你的 Pod 可以相互通信。 在安装网络之前，集群 DNS (CoreDNS) 将不会启动。**
>
> - 注意你的 Pod 网络不得与任何主机网络重叠： 如果有重叠，你很可能会遇到问题。 （如果你发现网络插件的首选 Pod 网络与某些主机网络之间存在冲突， 则应考虑使用一个合适的 CIDR 块来代替， 然后在执行 `kubeadm init` 时使用 `--pod-network-cidr` 参数并在你的网络插件的 YAML 中替换它）。
>
> - 默认情况下，`kubeadm` 将集群设置为使用和强制使用 [RBAC](https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/)（基于角色的访问控制）。 确保你的 Pod 网络插件支持 RBAC，以及用于部署它的 manifests 也是如此。
>
> - 如果要为集群使用 IPv6（双协议栈或仅单协议栈 IPv6 网络）， 请确保你的 Pod 网络插件支持 IPv6。 IPv6 支持已在 CNI [v0.6.0](https://github.com/containernetworking/cni/releases/tag/v0.6.0) 版本中添加。

> **说明：** kubeadm 应该是与 CNI 无关的，对 CNI 驱动进行验证目前不在我们的端到端测试范畴之内。 如果你发现与 CNI 插件相关的问题，应在其各自的问题跟踪器中记录而不是在 kubeadm 或 kubernetes 问题跟踪器中记录。

### 部署 flannel

[flannel](https://github.com/flannel-io/flannel) 是一个网络组件，可以为不同 node 节点分配不同的子网，实现容器间的跨机通信，从而实现整个 kubenets 层级通信。 由此可知，该组件运行于 node 节点上，依赖组件为 `etcd`，`docker`

先获取部署 flannel 的 yaml 文件，注意文件里面使用的镜像都是国外的，可以先尝试 pull 到本地，如果无法 pull，可以替换为阿里云的镜像，然后再部署。

```bash
$ wget https://github.com/flannel-io/flannel/blob/master/Documentation/kube-flannel.yml
$ kubectl apply -f kube-flannel.yml
$ kubectl get pods -n kube-system
NAME                            READY   STATUS    RESTARTS   AGE
coredns-6d8c4cb4d-2dhz4         1/1     Running   0          171m
coredns-6d8c4cb4d-flp79         1/1     Running   0          171m
etcd-gdj-1                      1/1     Running   4          171m
kube-apiserver-gdj-1            1/1     Running   3          171m
kube-controller-manager-gdj-1   1/1     Running   4          171m
kube-flannel-ds-dtdhd           1/1     Running   0          4m23s
kube-flannel-ds-gtsbp           1/1     Running   0          4m23s
kube-flannel-ds-wkm8q           1/1     Running   0          4m23s
kube-proxy-wxw2s                1/1     Running   0          149m
kube-proxy-x5qgk                1/1     Running   0          171m
kube-proxy-zqnl2                1/1     Running   0          149m
kube-scheduler-gdj-1            1/1     Running   4          171m
```

## 查看集群状态

查看集群组件状态

```bash
$ kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
etcd-0               Healthy   {"health":"true","reason":""}   
scheduler            Healthy   ok                              
controller-manager   Healthy   ok
```

查看节点状态

```bash
$ kubectl get node
NAME    STATUS   ROLES                  AGE     VERSION
gdj-1   Ready    control-plane,master   3h39m   v1.23.1
gdj-2   Ready    <none>                 3h17m   v1.23.1
gdj-3   Ready    <none>                 3h16m   v1.23.1
```

为节点添加 role 标签

```bash
$ kubectl label nodes gdj-1 node-role.kubernetes.io/worker=
node/gdj-1 labeled
$ kubectl label nodes gdj-2 node-role.kubernetes.io/worker=
node/gdj-2 labeled
$ kubectl label nodes gdj-3 node-role.kubernetes.io/worker=
node/gdj-3 labeled
$ kubectl get node
NAME    STATUS   ROLES                         AGE     VERSION
gdj-1   Ready    control-plane,master,worker   3h46m   v1.23.1
gdj-2   Ready    worker                        3h24m   v1.23.1
gdj-3   Ready    worker                        3h24m   v1.23.1
```

## 测试 kubernetes 集群

在 Kubernetes 集群中创建一个 pod，验证是否正常运行：

```bash
$ kubectl create deployment nginx --image=nginx
deployment.apps/nginx created
$ kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed
$ kubectl get pod,svc
NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-85b98978db-l57wg   1/1     Running   0          4m22s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        3h33m
service/nginx        NodePort    10.103.183.177   <none>        80:30089/TCP   101s
```

访问地址：http://NodeIP:Port